Using the kernel module "tg_module" to assess real-time group scheduling:

In rt_group_sched kernel:

Scenario #1:    parent calls sched_setscheduler() for SCHED_RR, before 
                forking a few children.

Scenario #2:    each process calls sched_setscheduler() for SCHED_RR in
                work() function individually


Scenario #3:    spawn a group of SCHED_OTHER procs, then assign two tasks 
                SCHED_RR, observe the task_group membership of all tasks 
                before and after the change.

1.  

In non-rt_group_sched kernel:

Scenario #1:    parent calls sched_setscheduler() for SCHED_RR, before 
                forking a few children.

Scenario #2:    each process calls sched_setscheduler() for SCHED_RR in
                work() function individually.

Scenario #3:    spawn a group of SCHED_OTHER procs, then assign two tasks 
                SCHED_RR, observe the task_group membership of all tasks 
                before and after the change.

1.  


Notes on trace files generated using the "no-check" kernel with cgroup v1
real-time group scheduling enabled. 

Using the "nprocs" program to test the "no-check" kernel:

1.  The use of the schedtool program to launch the processes causes a problem
    when there are 8 procs, 2 cgroups, and 2 cpus per cgroup. 
    
    If schedtool is envoked with SCHED_RR policy, then time-sharing among the 
    eight processes is not correct. Some processes will run for a lot longer 
    than others. This is true both before and after the tasks are placed into
    cgroups.
    
    When using "./nprocs 7" to investigate this behavior (no cgroups), it could 
    be seen that the first three nprocs processes were granted exclusive access 
    to the first three cpus, while the remaining five processes have the share 
    the last cpu equally.

    This problem can be replicated if sched_setscheduler() is called in the 
    parent before fork(). This problem is NOT due to the "reset-on-fork" 
    scheduling flag; each child indeed have SCHED_RR policy (see below):

    ---------------------------------------------------------------------------------------------------------
    # compile with sched_setschedular() before fork()ing children, in the initial parent process
    pi@muhanyupi:~/research/tools/nprocs $ gcc -o nprocs -DPMANUAL nprocs.c
    pi@muhanyupi:~/research/tools/nprocs $ ./nprocs 7
    pid: 13050
    pid: 13049
    pid: 13051
    pid: 13048
    pid: 13052
    pid: 13053
    pid: 13054
    pid: 13055
    ^C
    pi@muhanyupi:~/research/tools/nprocs $ 

    # check scheduling policy in another shell session (using schedtool leads to the same result)
    [root@muhanyupi /home/pi/research/tools/nprocs] # ps -e -o s,pid | grep ^R | awk '{system("chrt -p " $2)}'
    pid 12608's current scheduling policy: SCHED_OTHER
    pid 12608's current scheduling priority: 0
    pid 13048's current scheduling policy: SCHED_RR
    pid 13048's current scheduling priority: 90
    pid 13049's current scheduling policy: SCHED_RR
    pid 13049's current scheduling priority: 90
    pid 13050's current scheduling policy: SCHED_RR
    pid 13050's current scheduling priority: 90
    pid 13051's current scheduling policy: SCHED_RR
    pid 13051's current scheduling priority: 90
    pid 13052's current scheduling policy: SCHED_RR
    pid 13052's current scheduling priority: 90
    pid 13053's current scheduling policy: SCHED_RR
    pid 13053's current scheduling priority: 90
    pid 13054's current scheduling policy: SCHED_RR
    pid 13054's current scheduling priority: 90
    pid 13055's current scheduling policy: SCHED_RR
    pid 13055's current scheduling priority: 90
    chrt: failed to get pid 13123's policy: No such process
    chrt: failed to get pid 13124's policy: No such process
    ---------------------------------------------------------------------------------------------------------

    If the scheduling policy is instead changed with sched_setscheduler() in 
    the child processes after fork() (still using "./nprocs 7" to test), then 
    the load mis-balance problem is less frequent/severe. However, in some 
    cases, it still exists, where one core executes 1 proc, 2 cores execute 2 
    procs each, and the last core executes 3 procs.

    If we use the manual policy setting described above on the cgroup trials, we
    notice that the load mis-balance problem still exists when there are 8 procs,
    1 or 2 cgroups, and 1 or 2 cpus per cgroup. Mis-balance can happen before the
    procs were added to the cgroups in the same manner described above (1, 2, 
    2, 3 procs per cpu, respectively). Mis-balance was also observed after the 
    processes were added to the cgroups. Note that, by default, 
    cpuset.sched_load_balance flag for the cpuset controller is set, meaning 
    that the scheduler load-balances within each cpuset cgroup. In trials with 
    8 procs, 2 cgroups, and 2 cpus per cgroup, sometimes one proc uses one cpu 
    exclusively for several periods, while the other three procs have to share 
    the one remaining processor equally during the said periods. In trails with 
    8 procs, 1 cgroup, and 1 cpu per cgroup, the runtime is evenly divided between 
    8 procs equally, regardless of how the scheduling policy was manipulated.

    The same load-inbalance issue is also observed when using 4 procs, 1 cgroup, 
    and 2 cpus per cgroup. As before, inbalance only happens occasionally, not 
    always.
    
    Using 8 procs, 1 cgroup, and 2 cpus per cgroup, we observe that two cores 
    each execute 1 proc and two cores execute 3 procs. When added to cgroups, 
    time-sharing is similarly incorrect. However, incorrect time-sharing only 
    happen occasionally, NOT always.

    It is unclear why this is the case. 


2.  When there are 8 procs, 2 cgroups, and 2 cpus per cgroup, the vanila run_old.sh 
    cannot be used to effectively study the behavior of FIFO processes and their
    interactions with cgroups. Since there are only 4 cpus, only 4 processes will 
    run initially and write their PIDs to "procs_temp.txt". Therefore only these 
    4 processes will be added to the cgroup. Only after the 4 initial processes 
    were added to the cgroup, the 4 remaining processes that did not get to run 
    in the first place will have a chance to run, thereby writing their PIDs into
    "procs_temp.txt". 

    We can instead use a modified version run.sh, which checks exactly how many
    processes are the the cgroups right now, using the "tasks" file. The script 
    only stop adding more processes when the combined number of procs in those 
    tasks files is the same as the the number of processes initially spawned. 
    The only downside of this method is that it must repeatedly check those 
    files. Also, different procs might be added to their cgroups at very different 
    times.

    If using the the new run.sh script to run 8 procs, the behavior is as 
    expected. Each cpu adheres to the real-time runtime and period contraints, 
    running one FIFO process at a time.


Comparison with regular real-time group scheduling kernel:

1.  Load misbalancing is still observed, when testing with "./nprocs 7" and using 
    "manual" setting through sched_setscheduler(). As before, one core executes 1 
    proc, 2 cores execute 2 procs each, and the last core executes 3 procs. 
    
    In a rare case, two cores each execute 1 proc and 1 core execute 2 procs, 
    and 1 core executing 4 procs. This happens with 8 procs, 1 cgroup, 2 cpus 
    per cgroup. Although in this run, load-balancing between the two cpus in the 
    cgroup seems to be correct.

    At most times, loads across 4 cpus are balanced before adding to cgroups (2 
    procs per cpu).

    If scheduling policy is set through schedtool, similar load-misbalance is 
    observed, just as the no-check kernel -- three procs each grab one cpu 
    exclusively, while the remaining procs share a cpu. When added to cgroups, 
    time-sharing is also unbalanced.

2.  As before, load-misbalance is also observed.

    In the cgroup phase with 8 procs, 2 cgroups, 2 cpus per cgroup, in a cgroup 
    one cpu almost always execute 1 proc only, while the other cpu is divided 
    between the remaining 3 tasks.

    For a cgroup setup with 8 procs, 1 cgroup, 2 cpus per cgroup, load-balancing
    is sometimes correct (4 procs sharing one cpu equally). However, in some cases, 
    one cpu will be exclusively occupied by one task, while the rest has to share 
    the remaining cpu. 2-6, 3-5 situations are also observed.




Using Prof. Sudvarg's program to test the "no-check" kernel:

1.  

