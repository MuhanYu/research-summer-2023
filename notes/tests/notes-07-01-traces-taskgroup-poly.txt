Notes on trace files generated using the "no-check" kernel with cgroup v1
real-time group scheduling enabled. 

Using the "nprocs" program:

1.  The use of the schedtool program to launch the processes causes a problem
    when there are 8 procs, 2 cgroups, and 2 cpus per cgroup. 
    
    If schedtool is envoked with SCHED_RR policy, then time-sharing among the 
    eight processes is not correct. Some processes will run for a lot longer 
    than others. This is true both before and after the tasks are placed into
    cgroups.
    
    When using "./nprocs 7" to investigate this behavior (no cgroups), it could 
    be seen that the first three nprocs processes were granted exclusive access 
    to the first three cpus, while the remaining five processes have the share 
    the last cpu equally.

    This problem can be replicated if sched_setscheduler() is called in the parent 
    before fork(). This problem is NOT due to the "reset-on-fork" scheduling flag; 
    each child indeed have SCHED_RR policy (see below):

    ---------------------------------------------------------------------------------------------------------
    # compile with sched_setschedular() before fork()ing children, in the initial parent process
    pi@muhanyupi:~/research/tools/nprocs $ gcc -o nprocs -DPMANUAL nprocs.c
    pi@muhanyupi:~/research/tools/nprocs $ ./nprocs 7
    pid: 13050
    pid: 13049
    pid: 13051
    pid: 13048
    pid: 13052
    pid: 13053
    pid: 13054
    pid: 13055
    ^C
    pi@muhanyupi:~/research/tools/nprocs $ 

    # check scheduling policy in another shell session (using schedtool leads to the same result)
    [root@muhanyupi /home/pi/research/tools/nprocs] # ps -e -o s,pid | grep ^R | awk '{system("chrt -p " $2)}'
    pid 12608's current scheduling policy: SCHED_OTHER
    pid 12608's current scheduling priority: 0
    pid 13048's current scheduling policy: SCHED_RR
    pid 13048's current scheduling priority: 90
    pid 13049's current scheduling policy: SCHED_RR
    pid 13049's current scheduling priority: 90
    pid 13050's current scheduling policy: SCHED_RR
    pid 13050's current scheduling priority: 90
    pid 13051's current scheduling policy: SCHED_RR
    pid 13051's current scheduling priority: 90
    pid 13052's current scheduling policy: SCHED_RR
    pid 13052's current scheduling priority: 90
    pid 13053's current scheduling policy: SCHED_RR
    pid 13053's current scheduling priority: 90
    pid 13054's current scheduling policy: SCHED_RR
    pid 13054's current scheduling priority: 90
    pid 13055's current scheduling policy: SCHED_RR
    pid 13055's current scheduling priority: 90
    chrt: failed to get pid 13123's policy: No such process
    chrt: failed to get pid 13124's policy: No such process
    ---------------------------------------------------------------------------------------------------------

    If the scheduling policy is instead changed with sched_setscheduler() in 
    the child processes after fork() (still using "./nprocs 7" to test), then 
    the load mis-balance problem is less frequent/severe. However, in some 
    cases, it still exists, where one core executes 1 proc, 2 cores execute 2 
    procs each, and the last core executes 3 procs.

    If we use the manual policy setting described above on the cgroup trials, we
    notice that the load mis-balance problem still exists when there are 8 procs,
    1 or 2 cgroups, and 1 or 2 cpus per cgroup. Mis-balance can happen before the
    procs were added to the cgroups in the same manner described above (1, 2, 2, 3
    procs per cpu, respectively). Mis-balance was also observed after the processes
    were added to the cgroups. Note that, by default, cpuset.sched_load_balance 
    flag for the cpuset controller is set, meaning that the scheduler load-balances
    within each cpuset cgroup. In trials with 8 procs, 2 cgroups, and 2 cpus per 
    cgroup, sometimes one proc uses one cpu exclusively for several periods, while
    the other three procs have to share the one remaining processor equally during 
    the said periods. In trails with 8 procs, 1 cgroup, and 1 cpu per cgroup, the 
    runtime is evenly divided between 8 procs equally, regardless of how the scheduling
    policy was manipulated.

    The same load-inbalance issue is also observed when using 4 procs, 1 cgroup, 
    and 2 cpus per cgroup. As before, inbalance only happens occasionally, not always.

    It is unclear why this is the case. 


2.  When there are 8 procs, 2 cgroups, and 2 cpus per cgroup, the vanila run_old.sh 
    cannot be used to effectively study the behavior of FIFO processes and their
    interactions with cgroups. Since there are only 4 cpus, only 4 processes will 
    run initialially and write their pids to "procs_temp.txt". Therefore only these 
    4 processes will be added to the cgroup. Only after the 4 initial processes were
    added to the cgroup, the 4 remaining processes that did not get to run in the
    first place will have a chance to run, thereby writing their PIDs into
    "procs_temp.txt". 

    We can instead use a modified version run.sh, which checks exactly how many
    processes are the the cgroups right now, using the "tasks" file. The script only 
    stop adding more processes when the combined number of procs in those tasks files
    is the same as the the number of processes initially spawned. The only downside
    of this method is that it must repeatedly check those files. Also, different procs
    might be added to their cgroups at very different times.

    If using the the new run.sh script to run 8 procs, the behavior is as expected.
    Each cpu adheres to the real-time runtime and period contraints, running one
    FIFO process at a time.


Comparison with regular real-time group scheduling kernel:

1.  Load misbalancing is still observed, when testing with "./nprocs 7". As before,
    one core executes 1 proc, 2 cores execute 2 procs each, and the last core executes 
    3 procs.

2.  As before, load-misbalance is also observed when a cgroup has more than 1 proc and
    more than 1 cpu. 




Using Prof. Sudvarg's program to test the "no-check" kernel:

1.  

